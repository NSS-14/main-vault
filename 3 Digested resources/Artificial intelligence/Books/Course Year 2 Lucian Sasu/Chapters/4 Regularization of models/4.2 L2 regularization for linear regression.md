## Notes:

Let's suppose that the model is based on the polynomial function:

$$h_\theta(x) = \theta_0 + \theta_1x + \theta_2x^2 + \theta_3x^3 + \theta_4x^4$$


Intuitively, we want to put constrains on the coefficients $\theta_3, \theta_4$, they are multiplies with $x^3, x^4$, which determine a high variance of the function h. In other words, a small modification of the quantity x, makes big changes in the $h_\theta(x)$. The constrains are so, the absolute values of the $\theta_3, \theta_4$ to be as much close to 0.

For this, we will include inside the error function $J(\cdot)$ the squares of the $\theta_3, \theta_4$:

![[Draft_24-08-2023_15.56.38.excalidraw]]

The minimization of J will follow the decrising of the difference between the estimated values by the model and the real ones, but also the decrising of the absolut values of $\theta_3, \theta_4$. The example from the above is thinked to bring the polynomial function of the 4'th degree, close to a 2'nd degree one, for which we think it's generalize better.

In general we don't know which of the coefficients should have small absolute values. Intuitively, we realize that $\theta_0$ should not be necessary puted under constrains. We will impose constrains just to $\theta_1, \theta_2, ...$ to have small absolute values. Equivalent, their squares to be small, so the sum of their square to be not very big. The final scope is to avoid the overfitting. The principle is applied to independent variables as well, not just for those lose $size, size^2, ...$ . In other words, the above loss function can be rewrited as:

![[Draft_24-08-2023_17.17.10.excalidraw]]

where $\lambda > 0$ is the coefficient of the regularization term. The bigger $\lambda$ is, the more pronunced are the constraints imposed to the cofficients $\theta_1, \theta_2, ..., \theta_n$. At the extreme, if $\lambda \rightarrow \infty$, then $\theta_1, \theta_2, ..., \theta_n = 0$, and so $h_\theta(x) = \theta_0$, which is almost certain that means underfitting. The model returns always $\theta_0$, for all the inputs.

In matriceal form, the loss function including the regularization term is writen as:

![[Draft_24-08-2023_17.27.45.excalidraw]]

The algorith that searches by gradient descent becomes:

![[Draft_24-08-2023_17.39.32.excalidraw]]

We rewrite the modification of the $\theta_j$ from the above algorithm:

![[Draft_24-08-2023_18.51.35.excalidraw]]

We observe from the above that the  coefficients, $\theta_j$, suffer double modification: in the first place, it is decresed the gardient of the quality term times the rate of learning, but, in the second place, the coefficient, $\theta_j$ is scaled with an multiplicative term $1 - \alpha \lambda$, which, in the most cases, it is subunitar. We say in this case that we have an decay effect of the weights (weight decay). For the method gradient descent, L2 regularization is equivalent with weight decay. For other mothods of optimization, the afirmation is not valable.

For the normal equations, it can be shown that the regularization produces the following value for $\theta$:

![[Draft_24-08-2023_19.02.26.excalidraw]]
