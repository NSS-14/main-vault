## Notes:

### L2 regularization of the binary logistic regression:

For the binary logistic regression, the formula of the model is:

![[Draft_24-08-2023_19.09.23.excalidraw]]

If we permit to have non constrained weights, then the absolute value of them can grow and influence negatively the performance of generalization of the model: for small variances of the input we will have big variances in the values of the function. More over, if we consider input features of the form $x_i \cdot x_j, x_i \cdot x_j \cdot x_k, etc$, the model can aproximate very well the pairs from the training set, but without performing well on the test set. So, we prefer to constraint the weights, in such manner that they will be as small as posible in the absolute value.

For regularization, the loss function's form will be modified as:

![[Draft_24-08-2023_19.33.30.excalidraw]]

In vectorized form is writen as:

![[Draft_24-08-2023_19.39.40.excalidraw]]

The modifications that are bringed to the training algorithm is simple: we need to include the partial derivative of $\theta_i^2$ raported to $\theta_i$:

![[Draft_24-08-2023_19.45.01.excalidraw]]

The vectorized form of the training algorithm is:

![[Draft_24-08-2023_19.49.10.excalidraw]]

### L2 regularization for the multinomial logistic regression:

In practice, it is prefered to penalize the big absolute values of the weights $\theta_{ik} \space (1 \le i \le n, 1 \le k \le K)$; we don't penalize the weights of the free terms, $\theta_{0k} \space (1 \le k \le K)$. We add penalties for the squares of the remained weights and the loss function becomes:

![[Draft_24-08-2023_19.54.24.excalidraw]]

The formula used for all the gradients:

![[Draft_24-08-2023_20.00.03.excalidraw]]

The value of the hyperparameter $\lambda$ is determined with repeted tries, k fold cross-validation, random searching, Bayesian optimization, genetic algorithms, or other search euristics.