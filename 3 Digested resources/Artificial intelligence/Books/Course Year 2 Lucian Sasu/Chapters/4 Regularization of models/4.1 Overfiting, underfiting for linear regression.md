## Notes:

For the problem of estimating the price of a property, let's assume that there are **5** pairs in the training set. A pair being made up of the predictive variable, surface, denoted by **x**, and the output value, price. The **5** dots are represented below. Let's consider 3 models:

![[Draft_24-08-2023_14.53.19.excalidraw]]

Where for each case, x represents the surface; We say that we introduced new featues based on the existing ones, coresponding to the quntities $x^2, x^3, x^4$. First model is the linear one, discused previousely, the rest of them are polynomial (of degree 2 and 4). As before, there is the problem of finding out the coefficients $\theta_0, \theta_1, ..., \theta_{(last)}$. The model is determined also with the linear regression, based on multivariated data.

The graphs of the 3 polynomial models are:

![[Draft_24-08-2023_15.09.10.excalidraw]]

Intuitively, the first degree polynomial fails to make a very good approximation of the evolution of the price compared to the surface(size). We say that the model given by the first polynoimal suffers of "underfitting", being to simple for the problem. It can be said about such model that has "high bias", because makes simplists assumptions about the treated problem.

For the polynomial of degree 4, if no two prices are found on the same one
vertical (that is, we do not have two equal surfaces sold at different prices), the coefficients can be determined, so that the curve passes through
all 5 points (polynomial interpolation, see for example Lagrange polynomials). However, we note the non-monotonic shape, with large variations in the prediction, being cases where the estimated value decreases in relation to the surface. Here the model suffers from "overfitting", being too faithfully built on the data from the training set; if this data contains noise, then the model will perfectly learn the noise from the data set. Although the representation on the training data is perfect, the 4th degree polynomial even giving the known prices, otherwise it does not make a very credible prediction (we notice the price drop between the third and fourth point in the graph). The model is also said to have high variance, due to the fact that it is too complex for the problem
treated. The increased complexity is given by the large number of coefficients Î¸, resulting from the high degree of the approximation polynomial.

But, the second degree polynoimal presents the most credible form, even if it does not represents perfectly all the 5 pairs of values from the training set. It is a good compromise between the capacity to reproduce the training set and the capacity to generalize, this beeing the ability to calculate output values for cases that do not belong to the training set.

On short hand, the model which suffers of "underfitting" is incapable to represent the training set and to make estimation for other values. A model who suffers from "overfitting" can learn the training set, but he can't estimate very well input values outside the training set. In bouth cases, we say that the model does not generalize to good, generalizaiton beeing the capacity of a model to estimate close to the truth outsiede the cases that was used to train it. For the model, we are intresed about how good can he estimate outside the training set.

The exemplification was made starting from a predictive variable x, representing the size, which produces other features $x^2, x^3, x^4$. More generaly, we can suppose that we have input features defined in the problem domain, in our case, we can have: the distance between the property and utilities, poluation on the zone, etc. We must be able to cath the cases of "underfit" and "overfit" and treat them.

One way of treating the overfit is to reduce the number of features: for our problem, reduce the variables $x^3, x^4$. Another way to reduce it, is to choose an good model. And another one is to regularize the model: keeping the predictive variables, but puting constrains on them.