## Notes:

The scope of the algorithm is, starting from the training set, S, to obtain the weights w such that:

- for $x^{(i)} \in C_1$ to have $w^t \cdot x > 0$
- for $x^{(i)} \in C_2$ to have $w^t \cdot x < 0$

We will demonstrate that, if the sets $C_1$ and $C_2$ are linearly separable, then the algorithm will generate a linearly separable function.

The training starts with random seted weights. We denote this initial vector with w(1). With the help of an iterative process, we will obtain the weights w(2), ..., w(k), ... . The weights will stabilize at a given moment: w(l) = w(l+1) = ...

The learning(adaptation) of the weights w, it is made with the succesive adding of a difference vectors:

![[Draft_25-08-2023_10.19.35.excalidraw]]

For a current wights vector w(k), the current input $x^{(i)}$ and the associated label $y^{(i)}$, the estimation produced by the perceptron is $\hat y^{(i)} = f(w(k)^t \cdot x^{(i)})$.

If, for a vector of weights w(k) we have $\hat y^{(i)} = y ^{(i)}$ for all the training set, the the perceptron recognizes correct the classes given in S. Otherwise, it is necesarry to operate modifications on the current weights vector w(k), details will be given in the followings.

We consider the weights at the moment k, w(k). If, for a pair $(x^{(i)}, y^{(i)}) \in S$ we have $\hat y^{(i)} = y^{(i)}$, then for this case the weights vector does not need to be modified: perceptron classifies correct the input $x^{(i)}$. If $\hat y^{(i)} \neq y^{(i)}$, then we have one of the following cases:

1. $x^{(i)}$ is of the positive class, but is momentary classified as negative: $y^{(i)} = +1$ and $\hat y^{(i)} = -1$
2. $x^{(i)}$ is of the negative class, but is momentary classified as positive: $y^{(i)} = -1$ and $\hat y^{(i)} = +1$

For the case 1, we have $w(k)^t \cdot x^{(i)} < 0$, but we have to modify the product to be positive. So, we have to modify the vector w(k), such that, the new vector w(k+1) will have a greater product value: $w(k+1)^t \cdot x^{(i)} > w(k)^t \cdot x^{(i)}$, with the hope that $w(k+1)^t \cdot x^{(i)} > 0$ and $\hat y^{(i)} = f(w(k+1)^t \cdot x^{(i)}) = +1 = y^{(i)}$. The proposed modification is $\Delta w(k)$, which is:

$$\Delta w(k) = \alpha \cdot x^{(i)}$$

, where $\alpha$ is a striclty positve coefficient. The new weights vector will be: ^23ac8e

$$w(k+1) = w(k) + \Delta w(k) = w(k) + \alpha \cdot x^{(i)}$$

We verify that the vector w(k+1) brings a greater scalar product between the weights and the input vector:

![[Draft_25-08-2023_10.43.29.excalidraw]]

with strict inequality for $||x^{(i)}|| \neq 0$ (and because $x_0^{(i)} = 1$, we have that the norm of $x^{(i)}$ is strictly positive, so the inequality is, in fact, strictly). We obtain, so, an incrised scalar product, between the weights vector and the input vector, as we proposed.

Fot the case 2, we do the weights modification as follows:

$$\Delta w(k) = -\alpha \cdot x^{(i)}$$

so:

$$w(k+1) = w(k) + \Delta w(k) = w(k) -\alpha \cdot x^{(i)}$$

with the same condition for alpha:
![[5.3 Perceptron's training algorithm#^23ac8e]]
it verifies, similarly, that $w(k+1)^t \cdot x^{(i)} < w(k)^t \cdot x^{(i)}$:

![[Draft_25-08-2023_10.52.04.excalidraw]]

and because $\alpha$ and the norm of the vector $x^{(i)}$ are strictly positive, the inequality is strict.

Synthesising:

![[Draft_25-08-2023_10.57.11.excalidraw]]

So, we can rewrite the modifications of the weights as:

$$\Delta w(k) = \frac{\alpha}{2}(y^{(i)} - \hat y^{(i)}) \cdot x^{(i)}$$

It is posible that a single modification of the weights not to be sufficient for the whole training set to be right classified. If, during an epoch of training (epoch means to iterate the whole training set, once), appears at least one modification, then it will happen another epoch. The procces ends when the weights tabileze, so the training set is learned.

The training algorithm of the perceptron is:

![[Draft_25-08-2023_11.51.45.excalidraw]]

Regarthless the algorithm modifies instantaneously the weights for each case that the output given by the perceptron is not the same with the known label, the algorithm is not incremental, needing more epochs of training.

The fact that the algorithm stops after an finite amount of steps is demonstrated in the section 5.5.

After the algorithm stops, the classification of an vector to be in the negative or the postive class, is made like in section 5.2.
