## Notes:

![[Draft_25-08-2023_09.57.20.excalidraw]]

We can partition the input vectors $x^{(i)}$ in the subsets:

- $C_1 \text{ - positive class, those } x^{(i)} \in R^{n+1} \text{ for which } y^{(i)} = +1$
- $C_2 \text{ - negative class, those } x^{(i)} \in R^{n+1} \text{ for which } y^{(i)} = -1$

The set S, is given such way the sets $C_1 \text{ and } C_2$ are linearly separable, this beeing the condition in which the perceptron can learn to construct a separation surface.

The representation of a perceptron:

![[Draft_25-08-2023_10.04.11.excalidraw]]

The state value of the perceptron is $z = w^t \cdot x = \sum_{i=0}^n w_i \cdot x_i$. The output value (or the activation value) of the perceptron is calculated with the activation function:

![[Draft_25-08-2023_10.08.24.excalidraw]]

The value produced by the input x and the weights w:

![[Draft_25-08-2023_10.09.50.excalidraw]]

At inference made with weights w for a vector x:

- if $w^t \cdot x > 0$, then $f(w^t \cdot x) = 1$ and we say that the perceptron indicates that x is of the positive class.
- if $w^t \cdot x < 0$, then $f(w^t \cdot x) = -1$ and we say that the perceptron indicates that x is of negative class.

The value of the function f is undefined in 0: if $w^t \cdot x = 0$, then the pont x is on the separation surface, and it can't be said if he is of clas positive or negative.
