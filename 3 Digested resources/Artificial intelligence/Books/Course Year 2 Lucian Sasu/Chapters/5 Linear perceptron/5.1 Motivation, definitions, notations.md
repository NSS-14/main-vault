## Notes:

A linear perceptron is the simplest type of neuron, capable of learning to separate two classes of points that are linearly separable. The training is supervised.

### Definition:

![[Draft_25-08-2023_09.02.25.excalidraw]]

In the above conditions, we name the above class $C_1$, the positive class, and the class $C_2$, the negative class.

Starting from a training set formed formet with classes $C_1, C_2$ about we know that are linearly separable, but the function $y(\cdot)$ is unknown, the linear perceptron determines the coefficients $b, w_1, w_2, ..., w_n$ for whichi the condition from the definition above are satisfied.

Equation $y(x) = 0$, and the sets $C_1, C_2$ which y separates have simple geometric representations. The points $x \in R^n$ for which $y(x) = 0$ are forming an linear variaty, denoted with S, of dimension n - 1. If b = 0, then $y(x) = 0$ is an hyperplane (subspace of dimension n - 1 and passing throw origin), with the abusation of language, in literature it is used also hyperplane for the surface $y(x) = 0$, even if $b \neq 0$.

For the case n = 2, $w_1 > 0, w_2 > 0, b < 0$:

![[Draft_25-08-2023_09.26.46.excalidraw]]

The linearly surface $y(x) = 0$ separes the plane in two subsets: for the case n = 2, in two half-planes. Anyway we choose a point x from any subset, the sign of $y(x)$ is always the same, and if we go in the other subset, the sign will change. For n = 2, we talk about positive half-plane nad negative half-plane.

If $x_1 \space and \space x_2$ are two dots from the linear variaty S, then:

$$\displaylines{y(x_1) = 0 \Leftrightarrow w^t \cdot x_1 + b = 0 \\
y(x_2) = 0 \Leftrightarrow w^t \cdot x_2 + b = 0}$$

Substracting the second equation from the first one we obtain $w^t \cdot (x_1 - x_2) = 0$, or, equivalent, the vector w is perpendicular on the linear variety $y(x) = 0$:

![[Draft_25-08-2023_09.43.02.excalidraw]]

The distance between a pont of coordinates x and the surface S is:

$$z = \frac{|y(x)|}{||w||}$$

where w = $(w_1, ..., w_n)^t$ and ||w|| is the Euclidean norm of the vector w, $||w|| = \sqrt{w_1^2 + ... + w_n^2}$ .

We can renote the free term b, with $w_0$ and we can consider that the vector x has another compinent $x_0 = 1$. If we note with also with the w the extinded vector w = $(w_0 = b, w_1, ..., w_n)^t$ and x = $(x_0 = 1, x_1, ..., x_n)^t$, then the function y becomes:

$$y(x) = w^t \cdot x$$
