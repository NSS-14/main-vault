
## Topics:

## Notes:

### The ideea behind this method:

There is a method that gives the optimal values for $\theta^{(min)}$, based on an algebraic calculation.
The searched values for $\theta$ are those who produces the minimum of the J:

$$\theta^{(min)} = arg \space \underset{\theta \space \in \space R^{n+1}}{min} \space J(\theta)$$

Based on the Ferman's theorem, a necesary condition for $\theta^{(min)}$ to minimize the J, it is that the partial derivatives vector computed in $\theta^{(min)}$ to be the null vector:

$$\nabla_\theta J (\theta^{(min)}) = 0$$ ^c08342

, where 0 is the colum vector made with n+1 values of 0.

Because the function J is convex, the necesary minimization condition is also sufficient, so we will get the only minimum of J. If we use the formula: ![[Gradient vector#^9b391b]]
inside this formula: ![[2.4 Normal equations method#^c08342]]
we will get:

$$\displaylines{\frac{1}{2m}X^t(X\theta^{(min)} - Y) = 0 \\
X^t(X\theta^{(min)} - Y) = 0 \\
X^tX\theta^{(min)} - X^tY = 0 \\
X^tX\theta^{(min)} = X^tY}$$

which will define an equations system named "the normal equations". One step forward, if the $X^tX$ is not singular, the parameters vector, $\theta^{(min)}$ will be:

$$\theta^{(min)} = (X^tX)^{-1}X^t \cdot Y$$

### Pseudo-inverse:

The $(X^tX)^{-1}X^t$ is also called the Moore-Penrose pseudo-inverse, and it is also denoted with $X^+$. For an inversible matrix, the pseudo-inverse equals the inverse.

### If the $X^tX$ is singular:

- There are redundant features inside the data set.
- There are more features than lines in [[X]].

### Comparing this method with the previous one:
1. For this method we don't need to chose a good learning rate.
2. This method has just one iteration, the other method has many iterations.
3. The previous method works well on big data sets, but this method needs lot of resources to calculate the pdeudo-inverse.