## Notes:

### The training set:

In the case of binary logistic regression, the discrimination between two classes is followed. Classes are labeled with 1 - for the positive class, and 0 - for the negative class. The [[Training set]] and the [[Input vector]] are having the usual form:
![[Training set#^7df085]]
![[Input vector#^2c331e]]
And the:
![[Output value#^0f824d]]
$\in \{0, 1\}$, is the class that the object i, belongs to.

### The model's representation:

For the logistic regression, the model should produce a value which represents the conditioned probability P(class | [[Input vector]]). For this, we will use a [[Hypothesis]], $h_\theta(\cdot)$:

$$P(y = 1 | x; \theta) = h_\theta(x) = \frac{1}{1 + exp(-\theta^t \cdot x)} = \hat y$$

where $\theta$ is the column vector of coefficients:
![[Weights vector#^04c96a]]
It is easy to show that the values of the [[Hypothesis]] are from the interval $(0, 1)$, so we can use it as an probability. One step further, this probability represents the conditioned by the input([[Input vector]]) probability, and parametrized by the [[Weights vector]], $\theta$. P(y = 1 | x; $\theta$) is the trust factor which says if the object described by the vector x, belongs to the class 1, and the trust is also influenced by the $\theta$ vector.

The function that stays at the base of the [[Hypothesis]] is:

$$\sigma : R \rightarrow (0, 1),\space \sigma(z) = \frac{1}{1 + exp(-z)}$$

and is named "logistic sigmoid". We know that the values 0 and 1 are never toched by this function, we also know that is derivable, strictly incrising and the limit to the negative and positive infinity are 0, respectively 1. The naming "sigmoid", comes from the shape of the function, which reminds us of the letter "s".

### The graph of the logistic sigmoide definded by the above equation:

![[Draft_21-08-2023_22.51.08.excalidraw]]

### The properties of the sigmoid function:

$$\sigma (-z) = 1 - \sigma (z)$$

$$\displaylines{\sigma^{'} (z) = (\frac{1}{1 + exp(-z)})^{'} = \frac{-(1 + exp(-z))^{'}}{(1 + exp(-z))^2} = \frac{exp(-z)}{1 + 2exp(-z) + exp^2(-z)} \\
= \frac{1 - 1 + exp(-z)}{(1 + exp(-z))^2} = \frac{1}{1 + exp(-z)} - \frac{1}{(1 + exp(-z))^2} \\
= \sigma(z) (1 - \sigma(z))}$$

### The graph of the binary logistic regression model:

![[Draft_22-08-2023_13.40.17.excalidraw]]

### The probability that the object x belongs to a class:

The probability that the object x belongs to class 0, or the negative class, is P(y = 0 | x; $\theta$) and it is the complement against 1 of the event to be a positive class:

$$\displaylines{P(y=0|x;\theta) = 1 - P(y=1|x;\theta) = 1 - \frac{1}{1 + exp(-\theta^t \cdot x)} \\
= \frac{1 + exp(-\theta^t \cdot x)}{1 + exp(-\theta^t \cdot x)} - \frac{1}{1 + exp(-\theta^t \cdot x)} = \frac{exp(-\theta^t \cdot x)}{1 + exp(-\theta^t \cdot x)} \\
= \frac{e^{-\theta^t \cdot x}}{1 + e^{-\theta^t \cdot x}} = \frac{\frac{1}{e^{\theta^t \cdot x}}}{1 + \frac{1}{e^{\theta^t \cdot x}}} = \frac{\frac{1}{e^{\theta^t \cdot x}}}{\frac{1 + e^{\theta^t \cdot x}}{e^{\theta^t \cdot x}}} = \frac{1}{e^{\theta^t \cdot x}} \cdot \frac{e^{\theta^t \cdot x}}{1 + e^{\theta^t \cdot x}} \\
= \frac{1}{1 + e^{\theta^t \cdot x}} = \frac{1}{1 + exp(\theta^t \cdot x)} = \sigma(-\theta^t \cdot x)}$$

### What it is followed:

- for those [[Input vector]]s $x^{(i)}$, with associated label, $y^{(i)}$ = 1, we want $P(y^{(i)} = 1 | x^{(i)}; \theta)$ to be close to 1;
- for the vectors $x^{(i)}$ with $y^{(i)}$ = 0, $P(y^{(i)} = 0 | x^{(i)}; \theta)$ very close to 1, equivalent, we want $P(y^{(i)} = 1 | x^{(i)}; \theta)$ to be close to 0.

The coefficients from the $\theta$ vector will be determined with machine learning.

After the learning, the model is ussed for the classification in this way, if for an input vector **x**, we have:

$$\text{P(y = 1 | x; }\theta \text{)} \ge \text{P(y = 0 | x; }\theta \text{)}$$

then, the model will decide that the object described by the input vector x belongs to the positive class, 1, otherwise, it belongs to the negative class, 0.

We know that $\text{P(y = 0 | x; }\theta \text{)} = 1 - \text{P(y = 1 | x; }\theta \text{)}$, and because of that, we can rewrite the inequation from above:

$$\text{P(y = 1 | x; }\theta \text{)} \ge 0.5$$

### The vectorized calculation:

![[Pasted image 20230822143557.png]]

![[Pasted image 20230822143613.png]] ^594ac1

### The decision surface of the binary logistic regression:

![[Draft_22-08-2023_14.38.45.excalidraw]]

We obtaind an important result: if $\theta^t \cdot x \ge 0$, then x is classified to be the positive class, 1, otherwise, in the negative, 0 class. The classification between the two classes is made by the linear variety $\theta^t \cdot x = 0$. If x is in the positive part of the linear variety $\theta^t \cdot x = 0$ or on this variety, then he is in the class 1, otherwise is in the class 0.

If we allow in the vector x to be square, cubic, etc forms of the original features, then the decision surface will be more complicated. For example, we start from the vector ($x_1, x_2$), and we consider his extension with polinomial features: $x = (x_0 = 1, x_1, x_2, x_1^2, x_2^2, x_1 x_2)^t$; Then $\theta \in R^6$; We have $\theta^t \cdot x = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_1^2 + \theta_4 x_2^2 + \theta_5 x_1 x_2$. For the values $\theta_0 = -4, \space \theta_1 = \theta_2 = \theta_5 = 0, \space \theta_3 = \theta_4 = 1$, the decision surface will be $x_1^2 + x_2^2 = 4$, which represents a circle. Depending on the position relative to the circle(inside or outside of it). the object with coordinates $(x_1, x_2)^t$ is estimated to be a class or another class. We shown that the decision surface can be non-linear, if we introduce suplimentar features.

### The loss function:

The [[Loss function]] helps the model to find out, how good is a coefficients vector, $\theta$, and it is also used by the model to adjust the coefficients in the training proccess. The [[Loss function]] is denoted with $J(\cdot),\space J:R^{n+1} \rightarrow R_+$.
His argument is the [[Weights vector]], denoted by:
![[Weights vector#^04c96a]]
The value will be calculated on the [[Training set]], or on any set that contains pairs which contains [[Input vector]]s and associated output class.

Considering the mean of the number of good classification is not an option because the function is discrete, producing values from the set $\{0, \frac{1}{m}, \frac{2}{m}, ..., 1\}$, and we can't calculate partial derivatives on that.

Another way of defining the loss function is by using the loss function of the linear regression. But, because of the [[Hypothesis]] of the binary logistic regression, the loss function will not be convex any more, resulting in a search based on a gradient that can stop in a local minimum. We have down below an example of an non-convex function:

![[Draft_22-08-2023_17.55.00.excalidraw]]

We will use an error term, Cost($h_\theta(x^{(i)}), y^{(i)})$), and the loss function will be the mean of those terms:

$$J(\theta) = \frac{1}{m} \sum_{i = 1}^m Cost(h_\theta(x^{(i)}), y^{(i)})$$

The Cost($\cdot, \cdot$) function should require the following terms of dissimilarity:
1. the condition of close distance: if $h_\theta(x)$ and y are close values, then the value of the cost function should be very close to 0, and reciprocal.
2. the condition of big distance: if $h_\theta(x)$ and y are far from each other, then the value of the cost function should be big, and reciprocal.

### The definition of the cost function:

![[Draft_22-08-2023_22.35.46.excalidraw]]

### The graph of the cost function:

![[Draft_22-08-2023_22.40.16.excalidraw]]

### Rewriting the cost function:

$$\displaylines{Cost(h_\theta(x),y) = -y \cdot \text{ln} \space h_\theta(x) - (1 - y) \cdot \text{ln}(1 - h_\theta(x)) \\
= -y \cdot \text{ln} \space \hat y - (1 - y) \cdot \text{ln}(1 - \hat y)}$$

### Verify the two conditions about the distances:

We want to verify that:

$$Cost(h_\theta(x), y) \approx 0 \Leftrightarrow h_\theta(x) \approx y$$ 
For the case y = 1:

$$Cost(h_\theta(x),y) \approx 0 \Leftrightarrow - \text{ln} \space h_\theta(x) \approx 0 \Leftrightarrow h_\theta(x) \approx 1 = y \Leftrightarrow h_\theta(x) \approx y$$

For the case y = 0:

$$Cost(h_\theta(x),y) \approx 0 \Leftrightarrow -\text{ln}(1 - h_\theta(x)) \approx 0 \Leftrightarrow h_\theta(x) \approx 0 = y \Leftrightarrow h_\theta(x) \approx y$$

The first condition is satisfied.

Let's check the last condition. If $Cost(h_\theta(x),y) = M >> 0$, then we get the equivalent equality $h_\theta(x) = e^{-M}$ (y = 1), respectively, $h_\theta(x) = 1 - e^{-M}$ (y = 0); y is one of the boundries of the interva $[0,1]$, and if M is very big, then $h_\theta(x)$ goes towords the other boundry of the interval. Then the second and last condition is satisified as well.

In addition, because the both branches of the cost function are convex, results that the whole cost function is convex. Then, the loss function, J, as a sum of a finit number of convex functions, it is also convex. Then, any minimum point of J is guaranteed to be a global minimum.

The loss function is rewrited this way:

$$\displaylines{J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \cdot \text{ln} \space h_\theta(x^{(i)}) + (1 - y^{(i)}) \cdot \text{ln}(1 - h_\theta(x^{(i)}))] \\
= -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \cdot \text{ln} \space \hat y ^{(i)} + (1 - y^{(i)}) \cdot \text{ln}(1 - \hat y^{(i)})]}$$

And this form is named "binary cross-entropy".

If we consider the column vector $Y = (y^{(1)}, y^{(2)}, ..., y^{(m)})^t$ and the column vector $\hat Y$, from equation:
![[3.2 Binary logistic regression#^594ac1]]
Then we will rewrited the binary cross-entropy loss function as follows:

$$J(\theta) = -\frac{1}{m} (Y^t \cdot ln \space \hat Y + (1_m - Y)^t \cdot ln(1_m - \hat Y))$$

### The training algorithm:

The [[Training set]], is used for deduceing appropiate values for $\theta$, in such way that the value $h_\theta(x^{(i)})$ given by the model using the input vector $x^{(i)}$, to be as close as possible to the corresponding label $y^{(i)}$. Due to cost function properties, and to the fact that the loss function J represents the mean of the cost function applied on the training data set, we deduce that, by minimizeing the loss function J, we will get values $\hat y^{(i)} = h_\theta(x^{(i)})$ close to $y^{(i)}.$

To determine the $\theta^{(min)}$ who minimize the function J:

$$\theta^{(min)} = arg \space \underset{\theta \in R^{n+1}}{min} \space J(\theta)$$

we use the algorithm that searches by gradient descent. We start with random values of the vector $\theta$, or the null vector, and we modify them in iterative maner, decreasing at each step the gradient's value multiplied by a small coefficient $\alpha$, named the learning rate.

The training algorithm has the form:

1. Set the components of the $\theta$ to inital values, random, around 0 or even 0.
2. `repreat {
	$\theta_j = \theta_j - \alpha \cdot \frac{\partial}{\partial \theta_j} J(\theta)$ simultaneousely for all j
	`} until convergence`

The convergence can have the forms:

- The succesive values of the loss function are decresing but with a small amount $\varepsilon > 0$.
- The norm of the difference $L_2$ between two succesive values of the vector $\theta$ is under an amount $\varepsilon > 0$; in other words, the distance between two coefficient vectors becomes very small.
- an maximum interation amount.

The partial derivatives $\frac{\partial J}{\partial \theta_j}$ are:

$$\frac{\partial J}{\partial \theta_j}(\theta) = \frac{1}{m} \sum_{i=1}^m [(h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}] = \frac{1}{m} \sum_{i=1}^m[(\hat y^{(i)} - y^{(i)}) \cdot x_j^{(i)}]$$

And the modification inside the iteration becomes:

$$\theta_j = \theta_j - \alpha \cdot \frac{1}{m} \sum_{i=1}^m[(\hat y^{(i)} - y^{(i)}) \cdot x_j^{(i)}], \text{simultaneousely for all j}$$

We observe that the modification of the coefficients has the same form with the [[2.0 Linear regression]]. The only difference is at the [[Hypothesis]]: linear at linear regression, and sigmoidal at the logistic regression.

Simulaten attribuation can be done using temporar variables or using vectorized calculation. We can easy show that the gradient of J is:

$$\nabla_\theta J(\theta) = \frac{1}{m} X^t(\hat Y - Y)$$

,where $\hat Y$ is:
![[3.2 Binary logistic regression#^594ac1]]
and the modification inside the iterations becomes:
![[Pasted image 20230823112558.png]]

