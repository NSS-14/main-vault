## Notes:

![[Draft_27-08-2023_23.25.47.excalidraw]]

Let's consider that we have a data set for which we want to calculate the loss function. The input vectors are organized in the batch, where for all column vector $x^{(i)}$, we have an assigned label $y^{(i)}$.

The general form of the loss function for the the data set X with k labeled data:

![[Draft_27-08-2023_23.59.43.excalidraw]]

The factorization factor is here the regularisation $L_2$, a sum of Frobenius norms of the matrices $W^{[1]}, ..., W^{[L-1]}$:

![[Draft_28-08-2023_00.04.43.excalidraw]]

The regularisation puts presure on the weights, they should be directioned towords 0. An observation is that the bias weights are not regularized, that's why the vectors $b^{[l]}, ..., b^{[L-1]}$ does not appear in the regularization term. Same happened to the previous studied models.

With the Frobernius norm, the regularisation will be:

![[Draft_28-08-2023_00.08.25.excalidraw]]

### The cost function for regression problem:

![[Pasted image 20230828001116.png]]

, where:

![[Pasted image 20230828001155.png]]

In this case, the error function for the data set X with k pairs $(x^{(i)}, y^{(i)})$, including regularisation term becomes:

![[Draft_28-08-2023_00.12.57.excalidraw]]

The term $\frac{1}{k} \sum_{i=1}^k ||y^{(i)} - \hat y^{(i)}||^2$ it is named the mean square error (MSE). 

Another function is mean absolute error:

![[Draft_28-08-2023_12.21.48.excalidraw]]

A combination of the last two is the Huber function:

![[Draft_28-08-2023_12.24.26.excalidraw]]

The last three functions representation(for Huber, delta = 0.5):
![[Pasted image 20230828122705.png]]

### The cost function for determining between two classes:

If the problem has two classes, then the output layer has 1 neuron, with the sigmoid activation function. The loss function for a single pair (x, y) is:

$$J(W,b;x,y) = -y \space ln \space \hat y - (1 - y) \space ln(1 - \hat y)$$

, so the mean error function for the whole data X, with the regularisation term is:

![[Draft_28-08-2023_12.34.19.excalidraw]]

### The loss function for more than two independent classes:

For certain problems, it can be asked to determine if the current input has or belongs to some independent classes. For example, for an image it is asked if it contains or not object from class 1, class 2, ..., class m. We need a neural network with m output neurons, where neuron k gives the estimation that the input contains or belongs to class k.

The error function for a single pair (x, y) is:

![[Draft_28-08-2023_13.12.27.excalidraw]]

The loss function for the whole data X, with the regularisation term:

![[Draft_28-08-2023_13.15.02.excalidraw]]

### The loss function for classification between more than two classes:

For classification problems, it is preffered to use the error function cross-entropy, and in the output layer, the softmax activation function. The error function for a single pair is:

![[Draft_28-08-2023_13.18.39.excalidraw]]

And for the whole X is:

![[Draft_28-08-2023_13.21.15.excalidraw]]
