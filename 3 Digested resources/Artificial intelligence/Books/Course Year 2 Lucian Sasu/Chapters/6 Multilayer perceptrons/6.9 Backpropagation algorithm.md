![[Pasted image 20230828143539.png]]

The gradient descent algorithm:

![[Pasted image 20230828144327.png]]

![[Pasted image 20230828144334.png]]

$\alpha > 0$

### Ways to calculate it:

1. stochastic gradient descent: for every pair from the training data set $(x^{(k)}, y^{(k)})$ we calculate the value of the error junction J($W, b, x^{(k)}, y^{(k)}$), then the gradients and we apply the modifications for all the weights $w_{ij}^{[l]}$ and $b_i^{[l]}$. The next pair uses the new weights obtained at this step.
2. off-line or batch gradient descent: we calculate the gradients for the weights $w_{ij}^{[l]}$ and the biases $b_i^{[l]}$ for each pair from the training set; at final, we calculate the mean of them and update each weight, substracting that mean multiplied with the rate of learning, $\alpha$.
3. minibatch gradient descent: the training set is divided in disjoint sets(mini-batches), for example 100 of pairs $(x^{(k)}, y^{(k)})$. For each minibatch we calculate the mean of the gradients, we modify all the weights using the man multiplied by the rate of learning, then we go to the next batch.  In practice, this is the most used.

In all cases, passing throw all the training set, means an epoch, and passing throw a subset, is named an iteration. It is executed more training epochs, the stop condition may vary:

- the number of epoches riched a limit, for example 100.
- we follow the value of the error function on the training set and a disjoint set of it. If we see that the error on the validation set starts to increse, while on the trainig set still decresing, we hould stop.
- if the norms of the gradients are almost 0, it means that we reach a minimum point (locally or globally) or in a inflexion point and the weights will not get any more modifications.
- the error function drops under a certain level.

