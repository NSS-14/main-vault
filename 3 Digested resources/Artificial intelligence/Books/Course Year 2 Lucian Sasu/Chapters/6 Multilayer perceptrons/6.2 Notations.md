## Notes:

- p - the number of pairs from the training set
- $x^{(i)}$ - input vector from the training set, $x^{(i)} = (x_1^{(i)}, ..., x_n^{(i)})^t, 1 \le i \le p$
- $y^{(i)}$ - the output associated with the input $x^{(i)}$, from the training set, $y^{(i)} = (y_1^{(i)}, ..., y_m^{(i)}), 1 \le i \le p$
- L - the number of layers from the neural network, including the layers of input and output
- node - neuron if appears in the layer l, $1 \le l \le L-1$ or input node, if appears in the first layer with index 0
- $n_l$ - the number of neurons from the layer l, $0 \le l \le L-1$; $n_0 = n$; $n_{L-1} = m$
- $z_i^{[l]}$ - the state of the neuron i, from layer l, $1 \le i \le n_l$, $1 \le l \le L-1$
- $z^{[l]}$ - vector containing the states of the neurons from the layer l, $z^{[l]} = (z_1^{[l]}, ..., z_{n_l}^{[l]})^t, 1 \le l \le L-1$
- $a_i^{[l]}$ - the activation, the output value of the node i from layer l, $0 \le l \le L-1$, $1 \le i \le n_l$
- $a^{[l]}$ - the vector with activation from layer l
- $w_{ij}^{[l]}$ - the weight that links the neuron i from layer l and node j from layer l-1, $1 \le l \le L-1$, $1 \le i \le n_l$, $1 \le j \le n_{l-1}$
- $W^{[l]}$ - the weights between layer l and l-1 matrix, $1 \le l \le L-1$, $W_{ij}^{[l]} = w_{ij}^{[l]}$, $1 \le i \le n_l$, $1 \le j \le n_{l-1}$
- $W_i^{[l]}$ - line i from matrix $W^{[l]}$, $1 \le l \le L-1$, $1 \le i \le n_l$
- $b_i^{[l]}$ - the bias weight for neuron i and layer l, $1 \le l \le L-1$, $1 \le i \le n_l$
- $b^{[l]}$ - the vector of bias weights to layer l, $b^{[l]} = (b_1^{[l]}, ..., b_{n_l}^{[l]})$, $1 \le l \le L-1$
- $f^{[l]}$ - activation function for layer l, $1 \le l \le L-1$
- W - the sequence of weight matrices $(W^1, W^2, ..., W^{L-1})$
- b - sequence of weight vectors of bias $b^1, b^2, ..., b^{L-1}$
- J(W, b) - emperical mean error for set of vectors with known labels
- J(W, b; $x^{(i)}, y^{(i)}$) - error for the pair of vectors $(x^{(i)}, y^{(i)})$
- $\hat y^{(i)}$ - output column vector, corresponding to the vector $x^{(i)}$, calculated by the network
- $\delta^l$ - the vector with the error signal for the layer l, $1 \le l \le L-1$
- $\odot$ - Hadamard product