## Notes:

### Arhitecture:

There are more modes to disposition the neurons; we will use an multilayer type of arhitecture, feedforward, named multilayer perceptron - even if the used neurons are not perceptrons, but neurons with non-linear activation function. A multilayer network is composed with minimum 3 layers:

- input layer who takes the input values. It has no computational role, it is not formed with neurons.
- at least one hided layer, formed with neurons.
- output layer, composed with neurons, produces estimating values who are compared with the desired outputs.

The counting of the layers starts from 0. Neurons from the hided layer are producing new features based on the input vectors and to the non-linear activation functions, features that are then necesary for the neural network to produce estimations. It is possible that a network to have more than one neuron in the output layer, as we see in the next image:

![[Draft_25-08-2023_21.28.42.excalidraw]]
$$\displaylines{\text{MLP network with 4 layers,}\\
\text{that can be used for estimating two output values.}}$$

The training is more efficient if, additional to the input values and the calculated values by a layer of neurons, we bring a constant value, regulary +1, multiplied by a bias weight. The weights between layers and the bias weights are trainable, they will modify in the learning procces.

A representation of a neural network with 3 layers and one output is given in the image:

![[Draft_25-08-2023_21.42.05.excalidraw]]
$\displaylines{\text{MLP network with 3 layers. Can be used to estimate continuous values (regression problem)} \\ \text{or estimating the conditioned probabilities for two classes.}}$

There is no relation between the number of hided layers, and the number of neurons from those layers and the number of nodes of input and output.

We will consider that we have $L \ge 3$ layers and in each hidded layer l ($1 \le l \le L-1$), a number of $n_l$ nodes. The input layer has $n_0 = n$ nodes, the number of neurons from the output layer is $n_{L-1} = m$, given by the number of classes that we want to classify (for the classification problem or the conditioned probabilities estimations), respectively, the number of outputs that we want to aproximate (at the regression).

For the images above:

- the values $x_1, x_2, x_3$ are components of the input vector $x = (x_1, x_2, x_3)^t$. There is also considered an constant input with value +1, afferent to the bias coefficient.
- the value $a_i^{[l]}$ represents the activation of the node i from the layer l, $0 \le l \le L-1$, $1 \le i \le n_l$. For the input layer, $a_i^{[0]} = x_i$;
- the value $h_{W, b}(x)$ is the calculated output by the network for the current input vector x. $h_{W, b}(x) \in R$, for the last image and $h_{W, b}(x) \in R^2$ for the first image. The network from the last image, can be used to estimate continuous values (linear regression) or to estimate conditioned probabilities for two classes. The network from the first image can be used to estimate two output values, real numbers, regression problem.

Weights between node j from layer l-1 and nodes from layer l:

![[Draft_26-08-2023_12.47.51.excalidraw]]

Bias value and the weights from layer l-1 and the node i from layer l:

![[Draft_26-08-2023_12.52.30.excalidraw]]

The pair (W, b) is formed with the weights matrices and the bias coefficients from the network. We use the following notations:

- the weights between the input layer and the hiden layer are contained in the matrix $W^{[1]}: \space w_{ij}^{[1]}$ is the linking weight between the neuron i from layer 1(the second one), and the node j from layer 0(the first layer, the input layer). We remark, the order of the inferior indexes, further useful for the next operations;
- in general, we note $w_{ij}^{[l]}$ the linking weight between the node i from layer l and the node j from layer l-1, they form the weight matrix:$$W^{[l]} = \left [ \matrix{w_{11}^{[l]} & ... & w_{1n_{l-1}}^{[l]} \\ . & . & . \\
. & . & . \\
. & . & . \\
w_{n_l}^{[l]} & ... & w_{n_l n_{l-1}}^{[l]}} \right ]$$with $n_l \times n_{l-1}$ weights;
- the value of the bias weight between the constant input +1 from the input layer and the neuron i from the hidden layer is $b_i^{[l]}$, $1 \le i \le n_1$;
- in general, the bias weight from the layer l-1 ($1 \le l \le L-1$) and for the neuron i from layer l is denoted $b_i^{[l]}$, $1 \le i \le n_l$. The coefficients $b_i^{[l]}$ forms the column vector $b^{[l]}$ with $n_l$ coefficients.

Bevause of the fact that we have links between each two neurons from succesive layers, the MLP networks are also called "densely connected". There are other types of networks (for example convolutional ones, chapter 9) which uses less links.

### Activation functions:

Each neuron aggregates the values from the nodes from the previous layer, including the constant term +1, multiplying with the bias weight. The neuron of index i from the layer $l \ge 1$ has the calculated state as:

![[Draft_26-08-2023_13.25.12.excalidraw]]

, where: $W_i^{[l]}$ is the line i of the matrix $W^{[l]}$, $a_i^{[l-1]}$ is, by the case: the output value of the neuron i from the layer l-1 (if $l \ge 2$) or the value $x_i$ from the input vector (if l = 1). Obiously, the vector $a^{[l]}$ is $(a_1^{[l]}, ..., a_{n_l}^{[l]})^t$. Denoting with $z^{[l]}$ the column vector $(z_1^{[l]}, z_2^{[l]}, ..., z_{n_l}^{[l]})^t$, we can write in matriceal manner:

$$z^{[l]} = W^{[l]} \cdot a^{[l-1]} + b^{[l]}, \space 1 \le l \le L-1 $$

Based on the state $z_i^{[l]}$ of the neuron i from layer l , it is calculated his output (the activation), using the activation function $f^{[l]}$:

$$a_i^{[l]} = f^{[l]} (z_i^{[l]})$$

, for $1 \le l \le L-1, \space 1 \le i \le n_l$. If we use the notation:

$$f^{[l]} = ( (z_1, z_2, ..., z_k)^t ) = (f^{[l]}(z_1), f^{[l]}(z_2), ..., f^{[l]}(z_k))^t$$

, the function $f^{[l]}$ is applied on all the values of the argument vector. Then, we can rewrite the above equation:

$$a^{[l]} = f^{[l]} (z^{[l]})$$

The activation function $f^{[l]}(\cdot)$ is necessary in the forward propagation step.
For the backwards propagation his derivation is used. Popular chooses for the activation function:

1. Logistic sigmoidal function: $f = \sigma : R \to (0, 1), f(z) = \sigma(z) = \frac{1}{1 + e^{-z}}$; His derivative is: $\sigma'(z) = \sigma(z) \cdot (1-\sigma(z))$;
2. Hyperbolic tangent function: $f = tanh : R \to (-1,1), f(z) = tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$; And his derivative is: $tanh'(z) = 1 - tanh^2(z)$;
3. The linear function: $f(z) = a \cdot z + b$, with the derivative $f'(z) = a$; Frequiently, a = 1, b = 0, are choosed (identity function $f(z) = z$). It is choosed if the outputvalues are desired to be outside the limits (0, 1) and (-1, 1), as happens at those activation functions from above.
4. Softmax function: $softmax(z, i) = \frac{e^{z_i}}{\sum_{j=1}^m e^{z_j}}$; where i is the neuron's index, and m is the total number of values from the vector z. z is the vector with state values of the neurons, $z = (z_1, ..., z_m)^t$. The softmax function is used to transforme a vector of random values in probability distribution: it easy to show that softmax(z; c) $\in$ (0, 1) $\forall$ c and $\sum_{c=1}^{m} softmax(z; c) = 1$. In general, softmax is used for the output layer and the given values are interpreted convenably as the probability that the current input to be of class c, $1 \le c \le m$. The classification is made, finding that index $1 \le c \le m$ for which softmax(z; c) has the max value. It is used in the output layer of the neural network of classification or probability estimation. The partial derivatives of the softmax function are: ![[Draft_27-08-2023_14.09.23.excalidraw]]
5. Rectified Linear Unit (ReLU) function: $ReLU(z) = max(0, z)$; The derivatives on subintervals are easy to compute and has fast evaluation. In plus, they don't saturate as logistic sigmoid and hyperbolic tangent. The graphical representaion looks like this: ![[Draft_27-08-2023_14.21.37.excalidraw]], even if the function is linear of portions, it is non-linear taken as a whole. The fact that you can t calculate the derivative in one point does not affect its usage in the practice. The derivative of this function, in origin is taken to be 0, we have so: ![[Draft_27-08-2023_14.29.03.excalidraw]] Theoreticaly, there exists the probability that a big part of neurons which uses ReLU to produce activation 0, this is happening, for example, if the bias value is negative and very small. It is very probabily, that such neuron to become insensible to any input and produce always 0, we say about it that is a dying ReLU. If all the activations are 0, the network will produce a constant value,, for any input. Even at the learning we have problems, becoause the gradient for a dead ReLU is 0, so the neuron's weights does not modify.
6. Parametric ReLU function (PReLU): represents a generalization of the previous ReLU function:![[Draft_27-08-2023_14.35.10.excalidraw]]and $\alpha > 0$. The graph of the function with $\alpha = 0.1$ is:![[Draft_27-08-2023_14.43.26.excalidraw]]The derivatives on subintervals are:![[Draft_27-08-2023_14.53.08.excalidraw]]For $\alpha = 0.01$ we obtain a particular case, Leaky ReLU. PReLU solves the problem "dying ReLU", because permits the modification of the weights, even if the neuron is in a negative state.
7. The Exponential Linear Units (ELU) function:![[Draft_27-08-2023_14.56.22.excalidraw]]The graph of the function:![[Draft_27-08-2023_15.06.40.excalidraw]]The derivatives:![[Draft_27-08-2023_15.14.55.excalidraw]]
8. Swish function:![[Draft_27-08-2023_15.16.24.excalidraw]], where $\sigma$ is the logistic sigmoid, $\beta$ is a constant parameter or an trainable one. For networks with many layers, swish works better than ReLU. The represantion of the function is:![[Draft_27-08-2023_15.20.12.excalidraw]]

The list from above is not comprehensive. It is admited that the activation function to differ from layer to layer or from neuron to neuron. In practice, it is preferable to use the same activation function in the whole network, excepting, eventually, the last layer.