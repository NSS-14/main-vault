## Notes:

The multilayer neural networks or the multilayer perceptrons (MLP), are used for the regression, classification and conditioned probabilities problems. The training is supervised. They are one of the most popular type of artifical neural networks, and they belong to the forward propagation family.

In conformity with the last chapter, a linear perceptron is capable to find a hyperplane for separation two sets, if and only if, they are linearly separable. But, there are examples of problems that are of big interest and they are not linearly separable, and so not solvable by the linear perceptron, but they can be, somehow separated. In plus, we want to solve other of problems, excepting the binary classification: regression ( estimating values of output of continous type ), conditioned estimating, classificating any amount of calsses. This chapter has models based on neurons with functions of non-linear activation, in which, we can treat all of those types of problems.

In the previous chapter, it was gived an classic example of two sets which are not linearly separable, and so, a linear perceptron can't discriminate them. Another example is given by the figure:

![[Draft_25-08-2023_17.16.06.excalidraw]]
$$\text{Two non-linear separable classes}$$

We intuit that one neuron is to little for complex separation problems. At the same time, the concatenation of several neurons with a linear activation function is equivalent to the product between the input vector and a matrix resulting from the multiplication of a sequence of weight matrices. Due to the fact that matrix multiplication also produces a matrix, the operation is equivalent to multiplying a matrix by the input vector. We obtain from here that the sequence of neurons with linear activation function is equivalent to a single layer of neurons with linear activation function.

We will use, then, more neurons, and their activation functions will be non-linear.