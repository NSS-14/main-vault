## Notes:

Once the arhitecture of the network is setted up (the number of the hidded layers, the number of neurons, the activation functions) we can start training it and use it. The forward propagation step, takes an input vector $x = (x_1, ..., x_n)^t$ and produces modifications to the state of the network's neurons, starting from the input, succesively actioning to the layers 1, ..., L-1. This gives the name of the family from which the networks belong "the forward propagation / feedforward". The outputs from the last layer are used as the result: regression, conditioned probability estimations, classifications.

As affirmed before, the input layer has no computational role. His output value is the input vector, considered as column vector, x givven to network.

$$a^{[0]} = x$$

If we know the output values of the nodes from layer l-1 we can calculate the states of the neurons from layer l and the associated output values, as so:

![[Draft_27-08-2023_15.54.42.excalidraw]]

for l = 1, ..., L-1, with $f^{[l]}$ activation function. We will denote with $\hat y$ the vector with m output values produced by the network:

$$\hat y = a^{[L-1]}$$

For the case we work on a data set formed with pairs of input-output, the column input vectors can be concatenated horizontaly in a matrix, X. For example, for the set:

![[Draft_27-08-2023_16.07.45.excalidraw]]

The matrix X it is formed:

![[Draft_27-08-2023_16.14.54.excalidraw]]

In the following, we can have X as the matrix formed with all the data $x^{(1)}, ..., x^{(p)}$ from training set, or just a sample of them of k vectors (mini-batch). In particular, this mini-bath can be formed with just one vector, $x^{(i)}$. Forward propagation, assumes that the input X has k columns. The given input is forwardly propagated throw network and succesively transformed as:

![[Draft_27-08-2023_23.05.13.excalidraw]]

$Z^{[l]}$ and $A^{[l]}$ are matrices with $n_l$ lines and k columns.