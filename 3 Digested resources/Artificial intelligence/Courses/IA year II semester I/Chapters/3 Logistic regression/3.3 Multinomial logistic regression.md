## Notes:

### Introduction:

For the case where discrimination is required for K classes, K>2, regression logistic extends to construct K functions (models), simultaneously:

$$h_\Theta(x) = \left( \matrix{P(y = 1 |x; \Theta) \\ 
P(y = 2 |x; \Theta) \\
. \\
. \\
. \\
P(y = K |x; \Theta)} \right) = \left( \matrix{\hat y_1 \\
\hat y_2 \\
. \\
. \\
. \\
\hat y_K } \right) = \hat y$$

We compare between the output value of this multinomial model and the output value of the binary logistic regression: For the two classes is enough to use only one function, logistic sigmoid, producing a single value. For K > 2 classes, it is enough to build K-1 calsses, but for the comodity, it is preffered to make K functions, any of them can be accepted to be redundant: the sum of all K function is 1, any of them can be determined to be one minus the sum of the rest of the K-1 functions.

### The training set:

The training set suffers modification only for the values of the $y^{(i)}$, which now can belong to the set $\{1, ..., K\}$. In formal manner, the [[Training set]] is writen as:
![[Training set#^7df085]]
where the input vector $x^{(i)} \in R^{n+1}$ just like in the binary logistic regression, and $y^{(i)} \in \{1,...,K\}$.

### The softmax function:

![[Draft_23-08-2023_17.02.40.excalidraw]]

We will use this function to produce probabilities for an input vector x.

### Representing the model:

The weights who defines the classification model, are grouped inside a matrix, $\Theta \in R^{(n+1) \times K}$, constructed as:

$$\Theta = \left[ \matrix{| & | & | & | \\
\theta_1 & \theta_2 & ... & \theta_K \\
| & | & | & |} \right]$$

where the column vector $\theta_k \in R^{n+1}$ has the coefficients for the class k, $1 \le k \le K$.

The model:

![[Pasted image 20230823175829.png]]

for an input vector, $x \in R^{n+1}$.

The probability for an vector x, to be of class k ($1 \le k \le K$) is:

$$P(y=k|x;\Theta) = \frac{exp(\theta_k^t \cdot x)}{\sum_{l=1}^K exp(\theta_l^t \cdot x)} = \hat y_k \in (0,1)$$

For the classification of an vector x, it is calculated the probability of belonging to the class k, $P(y=k|x;\Theta)$ for all k between 1 and K, and it is choosed the $k^{(max)}$ which makes the maximum probability:

![[Pasted image 20230823180342.png]]

The prediction made by the model is that x belongs to the class $k^{(max)}$. An similar decision was made by the binary logistic regression.

The training is focused on determining $\Theta$ for which the model will recongnize the best the data from the training set. More clearely, if an object x from the training set is of class k, then we want $P(y=k|x;\Theta)$ to be very close to 1.

### The loss function:

As in the case of the binary logistic regression, we will quantify the difference of the calculated value by the model and the real value from the training data set. We introduce the function indicator $I(\cdot)$ which for an logic value returns 1 if it is true and 0 otherwise.

![[Draft_23-08-2023_18.16.09.excalidraw]]

The loss function of the multinomial logistic regression:

![[Draft_23-08-2023_18.17.41.excalidraw]]

And it is named the cross-entropy.

For the representation of the current class $y^{(i)}$ it used the codification "one-hot", so: if $y^{(i)} = k$, then his codification is $y_{ohe}^{(i)}$ which is acolumn vector with K values, having on the position k value 1 and 0 on the other positions. With this codification, the loss function is rewrited in more compacted as:

![[Draft_23-08-2023_18.31.58.excalidraw]]

Where the logharitm is applied on all the components of the vector $\hat y^{(i)}$.

### The vectorized way to calculate it:

The Hadamard product:

If A and B are two matrices of type $m \times n$, then the Hadamard product is the matrix C of type $m \times n$, $C = A \odot B, c_{ij} = a_{ij} \cdot b_{ij}, 1 \le i \le m, 1 \le j \le n$. If we consider the matrix $Y_{ohe}$, respectively $\hat Y$ of type $K \times m$, having each, the column i, the vector $y_{ohe}^{(i)}$, respectively   $\hat y^{(i)}$, then we can consider the matrix:

$$-\frac{1}{m} \cdot Y_{ohe} \odot ln \hat Y$$

for which we calculate the sum of all the elements:

If S is a matrix of type $m \times n$, then, the sum of its elements is:

$$\sum_{i=1}^m \sum_{j=1}^n S_{ij} = \mathbb{1}_m^t \cdot S \cdot \mathbb{1}_n$$

where $\mathbb{1}_p$ is the column vector containing p number of ones.
The loss function can be rewrited as:

$$J(\Theta) = -\frac{1}{m} \mathbb{1}_m^t \cdot (Y_{ohe} \odot ln \hat Y) \cdot \mathbb{1}_K$$