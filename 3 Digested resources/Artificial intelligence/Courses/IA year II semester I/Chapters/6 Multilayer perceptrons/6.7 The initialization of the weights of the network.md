## Notes:

The inital values of the weights W and b are randomly setted arround the value 0. It is necessary that the weights values not be all equal. If the weights will be all equal, all neurons from a layer will have the same state, because each neuron is linked with the same inputs as the others from the layer. Moreover, if the weights that are multiplied with the input are equal, then the activation value of each neuron from that layer is the same (the constant weight can be factorized). The argument is true starting from the first hidded layer. We will end in having neurons from same layer to compute the same value, which is useless and redundant. And, for the output layer, we will have equal values for all the output layers. The symmetry effect obtained with equal weights Wm can be eliminated with initalization of random values. The bias weights can be initialized with 0. The initialization with random values is enough to break the symmetry.