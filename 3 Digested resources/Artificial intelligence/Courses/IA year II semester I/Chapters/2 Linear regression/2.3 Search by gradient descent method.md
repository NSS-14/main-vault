## Topics:

## Notes:

### The ideea behind this method:
This is an iterative method, in which, it is made the minimization of the error function J. The ideea it is simple:
- start with initial values for $\theta_0, \space \theta_1$, setted to random values or 0;
- modify in iterative maner the current values of the parameters $\theta_0, \space \theta_1$ in such way that the error function J decreses it's value:
$$\displaylines{\theta_0 = \theta_0 - \alpha \cdot \frac{\partial J}{\partial \theta_0}(\theta_0, \theta_1) \\ \theta_1 = \theta_1 - \alpha \cdot \frac{\partial J}{\partial \theta_1}(\theta_0, \theta_1) \\ \text{It is important to operate the assignments in simultaneous way.} \\ \text{This can be achived using some temporary varaibles or using vectors:} \\ \left[ \begin{matrix} \theta_0 \\ \theta_1 \end{matrix} \right] = \left[ \begin{matrix} \theta_0 \\ \theta_1 \end{matrix} \right] - \alpha \space \cdot \space \left[ \begin{matrix} \frac{\partial J}{\partial \theta_0}(\theta_0,\theta_1) \\ \frac{\partial J}{\partial \theta_1}(\theta_0,\theta_1) \end{matrix} \right] \\ \text{If we use nabla, the formula looks like this:} \\ \nabla_\theta J = \left[ \begin{matrix} \frac{\partial J}{\partial \theta_0} \\ \frac{\partial J}{\partial \theta_1} \end{matrix} \right] \\ \theta = \theta - \alpha \space \cdot \space \nabla_\theta J(\theta_0, \theta_1)}$$

### The algorithm:
$$\displaylines{\text{repeat} \space \{ \\
\theta_j = \theta_j \space - \space \alpha \space \cdot \space \frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1), \space \text{simultaneously for j = 0, 1} \\
\} \space \text{until convergence}}$$

### The partial derivatives:
$$\frac{\partial}{\partial \theta_j}J(\theta_0, \theta_1) = \frac{1}{m} \sum_{i=1}^m [ \space (h_\theta(x^{(i)}) - y^{(i)}) \space \cdot \space x_j^{(i)} \space ]$$

### The formula for finding the coefficients:
$$\theta_j = \theta_j \space - \space \alpha \space \cdot \space \frac{1}{m} \sum_{i=1}^m [ \space (h_\theta(x^{(i)}) - y^{(i)}) \space \cdot \space x_j^{(i)} \space ] \space \text{simultaneously for j = 0, 1}$$

### Extension to multivariate input data:
1. The [[Input vector]], will look like this:
	$x^{(i)} = (x_0^{(i)} = 1, x_1^{(i)}, x_2^{(i)}, \space ... \space, x_n^{(i)})^t$.
2. The [[Hypothesis]](model) will become:
	$h_\theta(x) = \theta_0 + \theta_1 \cdot x_1 + \text{...} + \theta_n \cdot x_n = \theta^t \cdot x = x^t \cdot \theta$.
3. The error function J stays unmodified, but it will use the new model, $h_\theta$.
4. The equation from the search algorith will change:
	$\theta_j = \theta_j - \alpha \frac{\partial J}{\partial \theta_j}(\theta) = \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m [(h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}]$.
	simultaneously for j = $\overline{0,n}$.
### The graph of the multivariated linear regression model:
![[Draft_18-08-2023_14.50.39.excalidraw]]

### The multivariated model needs scaled input features:
For the multivariated model, it is recomanded to scale the input features to an interval, like [0, 1]. And, alternatively, standardizate the data, such way, that every feature will have the mean 0 and dispersion 1.

### Rewriting the loss function and the formula that modifies the coefficients:
We start with the new notation [[X]].