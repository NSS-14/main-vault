## Topics:
#ErrorFunction : [[2.2 Error function#The error function]];
#MeanSquareErrorFunction : [[2.2 Error function#Mean square error function]];
#MeanSquareErrorFunctionGraphs : [[2.2 Error function#The graphs of the mean square error function]], [[2.2 Error function#Another way to represent the error function]];
#FindingTheCoefficientsOfTheModel : [[2.2 Error function#Finding the best coefficients $ theta_0, space theta_1$]];

## Notes:

### How we determine the coefficients of the model:
There is an infinite ways to choose the coefficients of the model's equation.
The question is: how do we obtain the best coefficients for the model. A natural way of obtaining them, is by, determining in such way, that, the predicted values, $\hat{y}^{(i)} = h_\theta(x^{(i)})$, are, much as possible, close to the known values, $y^{(i)}$, for the training data set, S.

### The error function:
For all the values from the training data set, S, the error can be measured using the error function:
$$J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m}(\hat{y}^{(i)} \space - \space y^{(i)})^2 = \frac{1}{2m} \sum_{i=1}^{m}(h_\theta(x^{(i)}) \space - \space y^{(i)})^2$$

### Mean square error function:
$$MSE(\hat{y} - y) = \frac{1}{m} \sum_{i=1}^{m}(\hat{y}^{(i)} \space - \space y^{(i)})^2 = 2 \space \cdot J(\theta_0, \theta_1)$$

### The graphs of the mean square error function:
If we consider the function J, having the $\theta_0 = 0$, the function will be a function of degree 2, looking like that $J(0,\theta_1)$, depending only on one variable, $\theta_1$. The minimum of the function will be greater or equal to 0.
- The graph for the mean square error function with $\theta_0 = 0 \space and \space \theta_1 \space variable$:
![[Draft_11-08-2023_19.51.00.excalidraw]]
- The graph for the mean square error function with $\theta_0 \space and \space \theta_1 \space variable$:
![[Draft_11-08-2023_20.02.00.excalidraw]]

### Another way to represent the error function:
Based on some curves, the representation will be in a two dimensioned space (plane), having on each of the axis, $\theta_0 \space and \space \theta_1$ respectivelly. For a random value v, we consider the set containing all the pairs $(\theta_0, \space \theta_1)$, for which we obtain that error value, v. In other words, all the pairs, for which, $J(\theta_0, \space \theta_1) = v$. The result will be a set of curbes, similar with those in the image below. For the linear model, those curbes are eliptic and for big values of v, we get longer eliptics.
![[Draft_15-08-2023_13.55.33.excalidraw]]

### Finding the best coefficients $\theta_0, \space \theta_1$:
We have to find the best coefficients $\theta_0^{(min)}, \space \theta_1^{(min)}$, for which the error function has the minimum value it can have:
$$\displaylines{(\theta_0^{(min)}, \theta_1^{(min)})^t = arg \space \underset{(\theta_0,\theta_1)^t \space \in \space R^2}{min} J(\theta_0, \theta_1) = \\ = arg \underset{(\theta_0,\theta_1)^t \space \in \space R^2}{min} \space \frac{1}{2m} \sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2 = \\ = arg \underset{(\theta_0,\theta_1)^t \space \in \space R^2}{min} \space \sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2 = \\ = arg \underset{(\theta_0,\theta_1)^t \space \in \space R^2}{min} \space \sum_{i=1}^{m}(\hat y^{(i)} - y^{(i)})^2}$$
The coefficients $\theta_0, \space \theta_1$, for which the value of the error function J is the smallest, gives the best regresion model based on the training data set.