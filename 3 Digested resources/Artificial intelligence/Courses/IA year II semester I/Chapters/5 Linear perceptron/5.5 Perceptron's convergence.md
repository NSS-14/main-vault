## Notes:

We will demonstrate that the training algorithm of the perceptron ends after a finite amount of steps if the trainig set, S, is composed of the linearly separable classes, $C_1, C_2$.

The condition of linear separability permits us to say that exists a vector $w_* \in R^{n+1}$ with:

- for $x^{(i)} \in C_1$ to have $w_*^t \cdot x^{(i)} > 0$
- for $x^{(i)} \in C_2$ to have $w_*^t \cdot x^{(i)} < 0$

or:

$$y^{(i)} \cdot w_*^t \cdot x^{(i)} > 0, \text{ for all i}, 1 \leq i \leq m$$

We can say that the vector $w_*$ has the norm 1, otherwise, it can be made his normalization, and the above inequality stays true.

Because the training set is finite, we can find $\gamma > 0$ such that:

$$y^{(i)} \cdot w_*^t \cdot x^{(i)} > \gamma, \text{for all i}, 1\le i \le m$$

For example:

$$\gamma = \frac{1}{2} \cdot \underset{1 \le i \le m}{min} \{ y^{(i)} \cdot w_*^t \cdot x^{(i)} \} > 0$$

Because the vector $w_*$ is unknown (but we know it exists, because the classes are linearly separable), the quantity $\gamma$ is also unknown. His consideration is useful in the demonstration of the convergence of the perceptron's training algorithm.

And, also because of the fact that the training set is finite, we have that there exists R > 0, such that $||x^{(i)}|| \le R$: all the input vectors from the training set are contained inside a hyper-sphere, centered in the origin and with the radius R, big enouch to capture all of them. We can take R, as:

$$R = \underset{1 \le i \le m}{max} \space \{ ||x^{(i)}|| \}$$

Two assumptions that simplifies the demonstration theorem are:

1. $\alpha$ = 1, the value of alpha does not influences the algorithm as long it is positive.
2. initally, the vector w(1) has all the elements 0, ($w(1) = 0_{n+1}$).

### The perceptron convergent theorem:

The training algorithm makes less then $\frac{R^2}{\gamma^2}$ modifications on the weights, before returning a separation hyperplane.

Demonstration: For $k \ge 1$, let $x^{(i)}$ be a wrongly classified vector by the perceptron, that is $f(w(k)^t x^{(i)}) \neq y^{(i)}$, or, equivalent:

$$f(w(k)^t x^{(i)}) \neq y^{(i)} \Leftrightarrow y^{(i)} \cdot w(k)^t x^{(i)} < 0$$

For this situation of misclassification of a vector from the training set, the algorithm performs the modification of w(k). We will have:

![[Draft_25-08-2023_13.20.08.excalidraw]]

Using mathematical induction and knowing that w(1) = 0, it can be shown that:

![[Draft_25-08-2023_13.25.24.excalidraw]]

Using the inequality Cauchy-Schwartz:

$$|a^t \cdot b| \le ||a|| \cdot ||b||$$

and the fact that for every real number z:

$$z \le |z|$$

and we get:

![[Draft_25-08-2023_13.33.08.excalidraw]]

On the other hand, we have:

![[Draft_25-08-2023_13.35.34.excalidraw]]

Having in consideration that w(1) = $0_{n+1}$, with mathematical induction we show that:

$$||w(k+1)||^2 < kR^2$$

From the inequalities, it resultes:

$$k^2 \gamma^2 < ||w(k+1)||^2 \le kR^2$$

and so:

$$k < \frac{R^2}{\gamma^2} \space \square$$

We obtained that the index k that makes the modifications can't be as large as possible, so the algorithm stops in a finte time. His finalization also means obtaining a set of weights for the linear separation form which classifies well the cases from the training set.

Because the values of $\gamma$ is not known, the result from abouve does not say what is the maximum number of needed steps. But, there is a proof that the algorithm does not run infinetly. From the chriteria of finishing the algorithm, we deduce that at final, we have a perceptron that separates the $C_1$ and $C_2$.
